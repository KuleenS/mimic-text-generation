{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this only once - Sets up TF Eager execution.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable Eager execution - useful for seeing the generated data.\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 20:55:13.756133 140038208472896 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:780: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Setting a random seed.\n",
    "\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "\n",
    "# Set a seed so that we have deterministic outputs.\n",
    "RANDOM_SEED = 301\n",
    "trainer_lib.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run for setting up directories.\n",
    "\n",
    "import os\n",
    "\n",
    "# Setup and create directories.\n",
    "DATA_DIR = os.path.expanduser(\"/mimic/t2t/data\")\n",
    "OUTPUT_DIR = os.path.expanduser(\"/mimic/t2t/output\")\n",
    "TMP_DIR = os.path.expanduser(\"/mnt/\")\n",
    "\n",
    "# Create them.\n",
    "tf.gfile.MakeDirs(DATA_DIR)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "tf.gfile.MakeDirs(TMP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def sample_sentence():\n",
    "\n",
    "    return None\n",
    "\n",
    "def target_sentence(input_sentence):\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.utils import registry\n",
    "@registry.register_problem\n",
    "\n",
    "# We inherit from `Text2TextProblem` which takes care of a lot of details\n",
    "# regarding reading and writing the data to disk, what vocabulary type one\n",
    "# should use, its size etc -- so that we need not worry about them, one can,\n",
    "# of course, override those.\n",
    "class SortWordsAccordingToLengthRandom(text_problems.Text2TextProblem):\n",
    "\n",
    "    # START: Methods we should override.\n",
    "\n",
    "    # The methods that need to be overriden from `Text2TextProblem` are:\n",
    "    # `is_generate_per_split` and\n",
    "    # `generate_samples`.\n",
    "\n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "    # We have pre-existing data splits (train, eval, test) so we set\n",
    "    # this to True, which will have generate_samples be called for each of the\n",
    "    # dataset_splits.\n",
    "        return True\n",
    "\n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        # Here we are generating the data in-situ using the `sample_sentence`\n",
    "        # function, otherwise we would have downloaded the data and put it in\n",
    "        # `tmp_dir` -- and read it from that location.\n",
    "        del tmp_dir\n",
    "\n",
    "        # Unused here, is used in `Text2TextProblem.generate_data`.\n",
    "        del data_dir\n",
    "\n",
    "        # This would have been useful if `self.is_generate_per_split()` was True.\n",
    "        # In that case we would have checked if we were generating a training,\n",
    "        # evaluation or test sample. This is of type `problem.DatasetSplit`.\n",
    "        del dataset_split\n",
    "\n",
    "        # Just an arbitrary limit to our number of examples, this can be set higher.\n",
    "        MAX_EXAMPLES = 10\n",
    "\n",
    "        for i in range(MAX_EXAMPLES):\n",
    "            sentence_input = sample_sentence()\n",
    "            sentence_target = target_sentence(sentence_input)\n",
    "            yield {\n",
    "              \"inputs\"  : sentence_input,\n",
    "              \"targets\" : sentence_target,\n",
    "            }\n",
    "\n",
    "        # END: Methods we should override.\n",
    "\n",
    "        # START: Overridable methods.\n",
    "\n",
    "    @property\n",
    "    def vocab_type(self):\n",
    "        # We can use different types of vocabularies, `VocabType.CHARACTER`,\n",
    "        # `VocabType.SUBWORD` and `VocabType.TOKEN`.\n",
    "        #\n",
    "        # SUBWORD and CHARACTER are fully invertible -- but SUBWORD provides a good\n",
    "        # tradeoff between CHARACTER and TOKEN.\n",
    "        return text_problems.VocabType.SUBWORD\n",
    "\n",
    "    @property\n",
    "    def approx_vocab_size(self):\n",
    "        # Approximate vocab size to generate. Only for VocabType.SUBWORD.\n",
    "        return 2**13  # ~8k\n",
    "\n",
    "    @property\n",
    "    def dataset_splits(self):\n",
    "        # Since we are responsible for generating the dataset splits, we override\n",
    "        # `Text2TextProblem.dataset_splits` to specify that we intend to keep\n",
    "        # 80% data for training and 10% for evaluation and testing each.\n",
    "        return [{\n",
    "            \"split\": problem.DatasetSplit.TRAIN,\n",
    "            \"shards\": 8,\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.EVAL,\n",
    "            \"shards\": 1,\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.TEST,\n",
    "            \"shards\": 1,\n",
    "        }]\n",
    "\n",
    "        # END: Overridable methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_len_problem = SortWordsAccordingToLengthRandom()\n",
    "sort_len_problem.generate_data(DATA_DIR, TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager\n",
    "\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "# We can iterate over our examples by making an iterator and calling next on it.\n",
    "eager_iterator = tfe.Iterator(sort_len_problem.dataset(Modes.EVAL, DATA_DIR))\n",
    "example = eager_iterator.next()\n",
    "\n",
    "input_tensor = example[\"inputs\"]\n",
    "target_tensor = example[\"targets\"]\n",
    "\n",
    "# The tensors are actually encoded using the generated vocabulary file -- you\n",
    "# can inspect the actual vocab file in DATA_DIR.\n",
    "print(\"Tensor Input: \" + str(input_tensor))\n",
    "print(\"Tensor Target: \" + str(target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the encoders to decode the tensors to the actual input text.\n",
    "input_encoder = sort_len_problem.get_feature_encoders(\n",
    "    data_dir=DATA_DIR)[\"inputs\"]\n",
    "target_encoder = sort_len_problem.get_feature_encoders(\n",
    "    data_dir=DATA_DIR)[\"targets\"]\n",
    "\n",
    "input_decoded = input_encoder.decode(input_tensor.numpy())\n",
    "target_decoded = target_encoder.decode(target_tensor.numpy())\n",
    "\n",
    "print(\"Decoded Input: \" + input_decoded)\n",
    "print(\"Decoded Target: \" + target_decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
